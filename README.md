# ğŸ”¥ Minimal Transformer from Scratch â€“ Custom PyTorch Implementation

> "Inspire curiosity. Build from scratch." â€“ This project is a clean, minimal implementation of the original Transformer model from the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762), written completely from scratch in PyTorch.

---

## ğŸš€ Project Goal

This project was built to deeply understand the inner workings of Transformersâ€”without relying on high-level libraries like `transformers`. It captures every core component, including:

- Positional Encoding
- Scaled Dot-Product Attention
- Multi-Head Attention
- Encoder-Decoder structure
- Masking
- Label Smoothing (optional)
- Custom training loop with gradient clipping

---

## ğŸ§  Why I Built This

The purpose of this project was **to learn advanced deep learning concepts by starting from the basics**. Instead of relying on high-level tools, I chose to implement the Transformer architecture step-by-step from scratch to truly understand what happens under the hood.

> This is not just another Transformer repoâ€”it's a learning journey built with sweat, bugs, and a burning laptop fan.

---

## ğŸ› ï¸ Features

- âœ… Pure PyTorch implementation (no high-level huggingface shortcuts)
- âœ… Modularized code: easy to navigate and expand
- âœ… Works on small sequence-to-sequence tasks
- âœ… Ready-to-train and experiment

---
