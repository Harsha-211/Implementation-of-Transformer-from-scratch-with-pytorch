# 🔥 Minimal Transformer from Scratch – Custom PyTorch Implementation

> "Inspire curiosity. Build from scratch." – This project is a clean, minimal implementation of the original Transformer model from the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762), written completely from scratch in PyTorch.

---

## 🚀 Project Goal

This project was built to deeply understand the inner workings of Transformers—without relying on high-level libraries like `transformers`. It captures every core component, including:

- Positional Encoding
- Scaled Dot-Product Attention
- Multi-Head Attention
- Encoder-Decoder structure
- Masking
- Label Smoothing (optional)
- Custom training loop with gradient clipping

---

## 🧠 Why I Built This

The purpose of this project was **to learn advanced deep learning concepts by starting from the basics**. Instead of relying on high-level tools, I chose to implement the Transformer architecture step-by-step from scratch to truly understand what happens under the hood.

> This is not just another Transformer repo—it's a learning journey built with sweat, bugs, and a burning laptop fan.

---

## 🛠️ Features

- ✅ Pure PyTorch implementation (no high-level huggingface shortcuts)
- ✅ Modularized code: easy to navigate and expand
- ✅ Works on small sequence-to-sequence tasks
- ✅ Ready-to-train and experiment

---
